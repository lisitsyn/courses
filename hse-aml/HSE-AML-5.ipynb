{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import scipy.linalg as linalg\n",
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "import pandas as pd\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.datasets import load_iris\n",
    "from IPython.display import Markdown as md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Applied Machine Learning\n",
    "\n",
    "## Recomenders and embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recommender systems\n",
    "\n",
    "- The main goal: predict the rating (preference, score, ..) that a user $u$ would give to some item $i$\n",
    "- Important part of any massive online service\n",
    "- A huge field of study\n",
    "- Let us consider the classic user-movie setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Two approaches\n",
    "\n",
    "- Model user-item interaction directly (content-based)\n",
    "- Model user-user interaction to predict items (collaborative filtering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Content-based recommenders\n",
    "\n",
    "- The rating problem is cast to classification\n",
    "- Collect user actions (mouseovers, clicks, likes, whatever)\n",
    "- User features: genres user interacted with, countries of movies they did like, etc\n",
    "- Movie features: category, year, country, etc\n",
    "- Combine these two sets of features and predict if they match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Toy example\n",
    "\n",
    "We have a set of users with counts of each genre assigned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = [\n",
    "    {'comedy': 2, 'action': 1},\n",
    "    {'documentary': 1, 'action': 1}, \n",
    "    {'comedy': 3}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's vectorize them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1., 2., 0.],\n",
       "        [1., 0., 1.],\n",
       "        [0., 3., 0.]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vectorizer = DictVectorizer().fit(users)\n",
    "\n",
    "vectorizer.transform(users).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['action', 'comedy', 'documentary']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genres = vectorizer.feature_names_\n",
    "genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "features, labels = [], []\n",
    "for user in users:\n",
    "    for genre in genres:\n",
    "        label = +1 if genre in user else -1\n",
    "        user_features = np.ravel(vectorizer.transform(user).todense())\n",
    "        item_features = np.ravel(vectorizer.transform({genre: 1}).todense())\n",
    "        features.append(np.concatenate((user_features, item_features)))\n",
    "        labels.append(label)\n",
    "        \n",
    "features = np.vstack(features)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We've got a feature matrix and a labels vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features [[1. 2. 0. 1. 0. 0.]\n",
      " [1. 2. 0. 0. 1. 0.]\n",
      " [1. 2. 0. 0. 0. 1.]\n",
      " [1. 0. 1. 1. 0. 0.]\n",
      " [1. 0. 1. 0. 1. 0.]\n",
      " [1. 0. 1. 0. 0. 1.]\n",
      " [0. 3. 0. 1. 0. 0.]\n",
      " [0. 3. 0. 0. 1. 0.]\n",
      " [0. 3. 0. 0. 0. 1.]]\n",
      "Labels [ 1  1 -1  1 -1  1 -1  1 -1]\n"
     ]
    }
   ],
   "source": [
    "print('Features', features)\n",
    "print('Labels', labels.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You know the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression().fit(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simple content-based model\n",
    "\n",
    "- Our model obviously learns some tautology\n",
    "- Although it also tends to learn dependencies between genres\n",
    "- Given good features and good classifier such a model would produce quite good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32236604, 0.67763396]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict_proba(np.array([1, 0, 0, 1, 0, 0]).reshape(-1, 1).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.45985691, 0.54014309]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict_proba(np.array([1, 0, 0, 0, 0, 1]).reshape(-1, 1).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pitfalls\n",
    "\n",
    "- Train set is rather high-maintenance (you need proper positive and negative examples)\n",
    "- Feature engineering is very important\n",
    "- Scalability might become a problem\n",
    "- Can you leverage other users?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Collaborative filtering (CF)\n",
    "\n",
    "- Another approach for recommenders leveraging user-user relations\n",
    "- Could be user-centric or item-centric\n",
    "- User-centric: given a user, find other like-minded users and use their ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### User-item rating matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 4.],\n",
       "       [4., 0.]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likes = ('Lliane', 'Titanic', 4), ('Roxanne', 'Titanic', 5), ('Roxanne', 'Ghost', 4)\n",
    "users = {name: idx for (idx, name) in enumerate({x[0] for x in likes})}\n",
    "items = {name: idx for (idx, name) in enumerate({x[1] for x in likes})}\n",
    "\n",
    "R = np.zeros((len(users), len(items)))\n",
    "for user, item, score in likes:\n",
    "    R[users[user], items[item]] = score\n",
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Memory-based CF\n",
    "\n",
    "- The simplest approach to fill in missed ratings is:\n",
    "$\n",
    "R(user, item) = \\mathop{avg}_{\\text{item}~x} R(user, x)\n",
    "$\n",
    "- Let's try to clarify using other users:\n",
    "$R(user, item) = \\mathop{avg}_{\\text{item}~x} R(user, x) + \\mathop{avg}_{\\text{other users}~u} \\left(R(u, item) - \\mathop{avg}_{\\text{item}~x} R(u, x)\\right)$\n",
    "- Everybody likes Shawshank's Redemption so should you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Memory-base CF with similarity\n",
    "\n",
    "- What if we had a function $S$ to measure similarity between users?\n",
    "- The scoring rule would become:\n",
    "$\n",
    "R(user, item) = \\mathop{avg}_{\\text{item}~x} R(user, x) + \\frac{1}{\\text{normalizer}} \\sum_{\\text{other users}~u} S(user, u) \\left(R(u, item) - \\mathop{avg}_{\\text{item}~x} R(u, x)\\right)\n",
    "$\n",
    "- What is $S$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pearson similarity\n",
    "\n",
    "$S(u_1, u_2) = \\frac{\\sum_{item} \\left(R(u_1, item) - \\mathop{avg}_{x} R(u_2, x)\\right) \\left((R(u_2, item) - \\mathop{avg}_{x} R(u_2, x)\\right)}{\\sqrt{\\sum_{item} \\left(R(u_1, item) - \\mathop{avg}_{x} R(u_1, x)\\right)^2} \\sqrt{\\sum_{item}\\left(R(u_2, item) - \\mathop{avg}_{x} R(u_2, x)\\right)^2} }$\n",
    "\n",
    "- Sums are computed on items rated by both of the users \n",
    "- Measures if users $u_1$ and $u_2$ often agree on the high and the low ratings\n",
    "- Normalized using user's average rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Centricity\n",
    "\n",
    "- We considered a model that measures users similarity (user-centric)\n",
    "- Everything can be transposed to compute items similarity (item-centric)\n",
    "- There is no single solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Matrix factorization\n",
    "\n",
    "- A technique from linear algebra\n",
    "- The matrix $X$ can be factorized into a product of matrices with some properties\n",
    "- One of the most famous is LU and the other one is SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LU decomposition\n",
    "\n",
    "Useful to solve linear systems, $X=PLU$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P [[0. 1.]\n",
      " [1. 0.]]\n",
      "L [[1.         0.        ]\n",
      " [0.33333333 1.        ]]\n",
      "U [[3.         4.        ]\n",
      " [0.         0.66666667]]\n",
      "PLU [[1. 2.]\n",
      " [3. 4.]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1, 2], [3, 4]])\n",
    "P, L, U = linalg.lu(X)\n",
    "print('P', P)\n",
    "print('L', L)\n",
    "print('U', U)\n",
    "print('PLU', P.dot(L).dot(U))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Singular Value Decomposition\n",
    "\n",
    "Finds structure in matrix you don't see, $X=U\\Sigma V^{T}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [3., 4.]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U, s, Vh = linalg.svd(X, full_matrices=False)\n",
    "U.dot(np.diag(s)).dot(Vh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Low-rank SVD decomposition\n",
    "\n",
    "$U$, $\\Sigma$, $V^{T}$ are of the same size as $X$ but we can cut them and still get something reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.27357371, 1.80720735],\n",
       "       [2.87897923, 4.08528566]])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.outer(U[:, 0].dot(s[0]), Vh[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We lost some information but kept something basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Low-rank decomposition\n",
    "\n",
    "- We can find matrices $U, I$ such that $R \\sim U I$\n",
    "- $R$ is of size #users x #items\n",
    "- $U$ might be of size #users x $d$\n",
    "- $I$ might be of size #items x $d$\n",
    "- $d$ is a small number (3 - 10)\n",
    "- Such a decomposition is not exact but useful, it tries to capture the essential information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Embeddings\n",
    "\n",
    "- The $U$ matrix contains a vector for each user\n",
    "- The $I$ matrix contains a vector for each item\n",
    "- These vectors are also called embeddings\n",
    "- The idea to vectorize everything was so hot it is still so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2300000000000004"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = {\n",
    "    'Lliane': [9.3, 5.4, -3.2],\n",
    "    'Roxanne': [5.2, -1.2, 5.6],\n",
    "}\n",
    "items = {\n",
    "    'Titanic': [3.7, -0.5, 8.9],\n",
    "}\n",
    "score = np.array(users['Lliane']).dot(np.array(items['Titanic']))\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Real-world recommenders\n",
    "\n",
    "- Usually hybrid with a lot of running parts\n",
    "- Different ways to introduce contextual information\n",
    "- A lot of nuances to consider: train data, features, etc\n",
    "- Additional metrics like diversity and serendipity\n",
    "- Filter bubble and other problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PMI matrix\n",
    "\n",
    "- Pointwise mutual information: $\\log \\frac{P(u, v)}{P(u) P(v)}$\n",
    "- Measures how often $u, v$ happen together compared to discretely\n",
    "- Becomes an interesting tool if $u, v$ are some language's words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PMI matrix decomposition\n",
    "\n",
    "- It is long known that PMI matrix could provide useful insights on words\n",
    "- One obvious thing was to factorize it the same way we did with ratings\n",
    "- What if we had embeddings for words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word embeddings\n",
    "\n",
    "Similar to user-items setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.379999999999999"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = {\n",
    "    'Moscow': [4.2, 2.4],\n",
    "    'St. Petersburg': [-3.4, 2.3],\n",
    "    'New York': [-2.3, -3.1],\n",
    "    'Washington': [3.3, -2.7],\n",
    "}\n",
    "np.array(words['Moscow']).dot(np.array(words['Washington']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word embedding\n",
    "\n",
    "- Factorizing PMI sounds like a promising thing\n",
    "- Nobody knew how to do that properly and it didn't work well\n",
    "- The key was to consider PMI between words and \"contexts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word2vec\n",
    "\n",
    "- Introduced in 2013 and started a revolution\n",
    "- Small program made in totally unreadable C\n",
    "- Consists of two regimes: skip-gram and Continuous Bag of Words (CBoW)\n",
    "- Considers word contexts $\\dots, w_{i-3}, w_{i-2}, w_{i-1}, \\mathbf{w_{i}}, w_{i+1}, w_{i+2}, w_{i+3}, \\dots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Skip-gram\n",
    "\n",
    "- For any context, $w_i$ is fixed\n",
    "- We try to fit $w_{j}$ so that it is similar to $w_i$\n",
    "- Learn to predict the context given a word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Continuous Bag of Words\n",
    "\n",
    "- For any context, everything but $w_i$ is fixed\n",
    "- We try to fit vector of $w_i$ to the vector of context\n",
    "- Learn to predict the word given a context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What do we get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.25585938, -0.02209473,  0.02905273,  0.05444336, -0.07421875,\n",
       "        0.35351562, -0.06347656,  0.14453125,  0.07226562,  0.10009766],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_vector('machine')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Algebra of vectors\n",
    "\n",
    "The vectors we learn in word2vec are actually following some algebraic rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('germany', 0.48318061232566833),\n",
       " ('korea', 0.4527997374534607),\n",
       " ('european', 0.4290318191051483),\n",
       " ('asia', 0.42432671785354614),\n",
       " ('german', 0.4178942143917084),\n",
       " ('enregistrer', 0.41728442907333374),\n",
       " ('charles', 0.41599637269973755),\n",
       " ('facto', 0.41378772258758545),\n",
       " ('vietnam', 0.40711793303489685),\n",
       " ('eu', 0.40642309188842773)]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['berlin', 'russia'], negative=['moscow'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning embeddings\n",
    "\n",
    "- Once we have the objective (skip-gram or CBoW) it is possible to employ optimization\n",
    "- Optimization refines the embeddings so that they fit together better\n",
    "- In the end, you get a lookup table (word -> vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### fastText\n",
    "\n",
    "- Very similar thing to word2vec\n",
    "- Consider character n-grams \n",
    "- This enables capturing some language structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cha', 'har', 'ara', 'rac', 'act', 'cte', 'ter']"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'character'\n",
    "[word[i:i+3] for i in range(len(word) - 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PMI?\n",
    "\n",
    "- All the methods for finding word embeddings are doing something similar\n",
    "- It is some equivalent of the PMI matrix factorization\n",
    "- Not really an important issue for practicioners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word embeddings in practice\n",
    "\n",
    "- One doesn't really need to train word embeddings\n",
    "- Just google 'pretrained word2vec' or 'pretrained fasttext' and enjoy the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Practical considerations\n",
    "\n",
    "- To embed sentences just sum the word embeddings\n",
    "- Do not forget to normalize the embeddings\n",
    "- Each component of embedding is important, hints for neural networks and linear models\n",
    "- You can actually embed URLs the same way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Next class\n",
    "\n",
    "- Non-parametric methods\n",
    "- k nearest neighbor\n",
    "- k-means"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
