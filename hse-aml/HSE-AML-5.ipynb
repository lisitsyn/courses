{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import scipy.linalg as linalg\n",
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.datasets import load_iris\n",
    "from IPython.display import Markdown as md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Applied Machine Learning\n",
    "\n",
    "## Recommenders and embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recommender systems\n",
    "\n",
    "- The main goal: predict the rating (preference, score, ..) that a user $u$ would give to some item $i$\n",
    "- Important part of any massive online service\n",
    "- A huge field of study\n",
    "- Let us consider the classic user-movie setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Two approaches\n",
    "\n",
    "- Model user-item interaction directly (content-based)\n",
    "- Model user-user interaction to predict items (collaborative filtering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Content-based recommenders\n",
    "\n",
    "- The rating problem is cast to classification\n",
    "- Collect user actions (mouseovers, clicks, likes, whatever)\n",
    "- User features: genres user interacted with, countries of movies they did like, etc\n",
    "- Movie features: category, year, country, etc\n",
    "- Combine these two sets of features and predict if they match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Toy example\n",
    "\n",
    "We have a set of users with counts of each genre assigned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = [\n",
    "    {'comedy': 2, 'action': 1},\n",
    "    {'documentary': 1, 'action': 1}, \n",
    "    {'comedy': 3}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's vectorize them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1., 2., 0.],\n",
       "        [1., 0., 1.],\n",
       "        [0., 3., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vectorizer = DictVectorizer().fit(users)\n",
    "\n",
    "vectorizer.transform(users).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['action', 'comedy', 'documentary']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genres = vectorizer.feature_names_\n",
    "genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "features, labels = [], []\n",
    "for user in users:\n",
    "    for genre in genres:\n",
    "        label = +1 if genre in user else -1\n",
    "        user_features = np.ravel(vectorizer.transform(user).todense())\n",
    "        item_features = np.ravel(vectorizer.transform({genre: 1}).todense())\n",
    "        features.append(np.concatenate((user_features, item_features)))\n",
    "        labels.append(label)\n",
    "        \n",
    "features = np.vstack(features)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We've got a feature matrix and a labels vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features [[1. 2. 0. 1. 0. 0.]\n",
      " [1. 2. 0. 0. 1. 0.]\n",
      " [1. 2. 0. 0. 0. 1.]\n",
      " [1. 0. 1. 1. 0. 0.]\n",
      " [1. 0. 1. 0. 1. 0.]\n",
      " [1. 0. 1. 0. 0. 1.]\n",
      " [0. 3. 0. 1. 0. 0.]\n",
      " [0. 3. 0. 0. 1. 0.]\n",
      " [0. 3. 0. 0. 0. 1.]]\n",
      "Labels [ 1  1 -1  1 -1  1 -1  1 -1]\n"
     ]
    }
   ],
   "source": [
    "print('Features', features)\n",
    "print('Labels', labels.T)\n",
    "\n",
    "# positives: likes/interactions/etc (scarce, not a lot of them) ~ millions\n",
    "# negatives: in na√Øve ~ billions, find out good negatives ~ millions \n",
    "\n",
    "#user = [6.2, 4.5, ...] <- decision trees, neural networks, ...\n",
    "#item = [1.2, 4.1, ...]\n",
    "#[user + item], label "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You know the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression().fit(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simple content-based model\n",
    "\n",
    "- Our model obviously learns some tautology\n",
    "- Although it also tends to learn dependencies between genres\n",
    "- Given good features and good classifier such a model would produce quite good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict(np.array([1, 0, 0, 0, 0, 1]).reshape(-1, 1).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40921065, 0.59078935]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict_proba(np.array([1, 0, 0, 0, 0, 1]).reshape(-1, 1).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pitfalls\n",
    "\n",
    "- Train set is rather high-maintenance (you need proper positive and negative examples)\n",
    "- Feature engineering is very important\n",
    "- Scalability might become a problem\n",
    "- Can you leverage other users?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Collaborative filtering (CF)\n",
    "\n",
    "- Another approach for recommenders leveraging user-user relations\n",
    "- Could be user-centric or item-centric\n",
    "- User-centric: given a user, find other like-minded users and use their ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### User-item rating matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 4.],\n",
       "       [4., 5.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likes = ('Lliane', 'Titanic', 4), ('Roxanne', 'Titanic', 5), ('Roxanne', 'Ghost', 4)\n",
    "users = {name: idx for (idx, name) in enumerate({x[0] for x in likes})}\n",
    "items = {name: idx for (idx, name) in enumerate({x[1] for x in likes})}\n",
    "\n",
    "R = np.zeros((len(users), len(items)))\n",
    "for user, item, score in likes:\n",
    "    R[users[user], items[item]] = score\n",
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Memory-based CF\n",
    "\n",
    "- The simplest approach to fill in missed ratings is:\n",
    "$\n",
    "R(user, item) = \\mathop{avg}_{\\text{item}~x} R(user, x)\n",
    "$\n",
    "- Let's try to clarify using other users:\n",
    "$R(user, item) = \\mathop{avg}_{\\text{item}~x} R(user, x) + \\mathop{avg}_{\\text{other users}~u} \\left(R(u, item) - \\mathop{avg}_{\\text{item}~x} R(u, x)\\right)$\n",
    "- Everybody likes Shawshank's Redemption so should you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 4.],\n",
       "       [4., 5.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R\n",
    "\n",
    "Rating(Lianne, Titanic) = 4 (Lianne's usual rating) + avg(-0.5, 0, -0.5) = 3 2/3\n",
    "\n",
    " Lianne  Roxanne   John    Ann\n",
    "[0,      4,        1,      3\n",
    " 4,      5,        1,      4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Memory-base CF with similarity\n",
    "\n",
    "- What if we had a function $S$ to measure similarity between users?\n",
    "- The scoring rule would become:\n",
    "$\n",
    "R(user, item) = \\mathop{avg}_{\\text{item}~x} R(user, x) + \\frac{1}{\\text{normalizer}} \\sum_{\\text{other users}~u} S(user, u) \\left(R(u, item) - \\mathop{avg}_{\\text{item}~x} R(u, x)\\right)\n",
    "$\n",
    "- What is $S$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pearson similarity\n",
    "\n",
    "$S(u_1, u_2) = \\frac{\\sum_{item} \\left(R(u_1, item) - \\mathop{avg}_{x} R(u_1, x)\\right) \\left((R(u_2, item) - \\mathop{avg}_{x} R(u_2, x)\\right)}{\\sqrt{\\sum_{item} \\left(R(u_1, item) - \\mathop{avg}_{x} R(u_1, x)\\right)^2} \\sqrt{\\sum_{item}\\left(R(u_2, item) - \\mathop{avg}_{x} R(u_2, x)\\right)^2} }$\n",
    "\n",
    "- Sums are computed on items rated by both of the users \n",
    "- Measures if users $u_1$ and $u_2$ often agree on the high and the low ratings\n",
    "- Normalized using user's average rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Centricity\n",
    "\n",
    "- We considered a model that measures users similarity (user-centric)\n",
    "- Everything can be transposed to compute items similarity (item-centric)\n",
    "- There is no single solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Matrix factorization\n",
    "\n",
    "- A technique from linear algebra\n",
    "- The matrix $X$ can be factorized into a product of matrices with some properties\n",
    "- One of the most famous is LU and the other one is SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LU decomposition\n",
    "\n",
    "Useful to solve linear systems, $X=PLU$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P [[0. 1.]\n",
      " [1. 0.]]\n",
      "L [[1.         0.        ]\n",
      " [0.33333333 1.        ]]\n",
      "U [[3.         4.        ]\n",
      " [0.         0.66666667]]\n",
      "PLU [[1. 2.]\n",
      " [3. 4.]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1, 2], [3, 4]])\n",
    "P, L, U = linalg.lu(X)\n",
    "print('P', P)\n",
    "print('L', L)\n",
    "print('U', U)\n",
    "print('PLU', P.dot(L).dot(U))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Singular Value Decomposition\n",
    "\n",
    "Finds structure in matrix you don't see, $X=U\\Sigma V^{T}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.40455358, -0.9145143 ],\n",
       "        [-0.9145143 ,  0.40455358]]),\n",
       " array([5.4649857 , 0.36596619]),\n",
       " array([[-0.57604844, -0.81741556],\n",
       "        [ 0.81741556, -0.57604844]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U, s, Vh = linalg.svd(X, full_matrices=False)\n",
    "U.dot(np.diag(s)).dot(Vh)\n",
    "U, s, Vh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Low-rank SVD decomposition\n",
    "\n",
    "$U$, $\\Sigma$, $V^{T}$ are of the same size as $X$ but we can cut them and still get something reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.27357371, 1.80720735],\n",
       "       [2.87897923, 4.08528566]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.outer(U[:, 0].dot(s[0]), Vh[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We lost some information but kept something basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Low-rank decomposition\n",
    "\n",
    "- We can find matrices $U, I$ such that $R \\sim U I$\n",
    "- $R$ is of size #users x #items\n",
    "- $U$ might be of size #users x $d$\n",
    "- $I$ might be of size #items x $d$\n",
    "- $d$ is a small number (3 - 10)\n",
    "- Such a decomposition is not exact but useful, it tries to capture the essential information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Embeddings\n",
    "\n",
    "- The $U$ matrix contains a vector for each user (4.5, -2.3, 6.7)\n",
    "- The $I$ matrix contains a vector for each item (-9.8, 7.5, 1.2)\n",
    "- These vectors are also called embeddings\n",
    "- The idea to vectorize everything was so hot it is still so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.709999999999996"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = {\n",
    "    'Lliane': [9.7, 5.4, -3.2],\n",
    "    'Roxanne': [0.9, 0.1, 0.2],\n",
    "}\n",
    "items = {\n",
    "    'Titanic': [3.7, -0.5, 8.9],\n",
    "    'Other movie': [...]\n",
    "}\n",
    "sorted(np.array(users['Lliane']).dot(np.array(items[x])) for x in items)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Real-world recommenders\n",
    "\n",
    "- Usually hybrid with a lot of running parts\n",
    "- Different ways to introduce contextual information\n",
    "- A lot of nuances to consider: train data, features, etc\n",
    "- Additional metrics like diversity and serendipity\n",
    "- Filter bubble and other problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PMI matrix\n",
    "\n",
    "- Pointwise mutual information: $\\log \\frac{P(u, v)}{P(u) P(v)}$\n",
    "- Measures how often $u, v$ happen together compared to discretely\n",
    "- Becomes an interesting tool if $u, v$ are some language's words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PMI matrix decomposition\n",
    "\n",
    "- It is long known that PMI matrix could provide useful insights on words\n",
    "- One obvious thing was to factorize it the same way we did with ratings\n",
    "- What if we had embeddings for words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word embeddings\n",
    "\n",
    "Similar to user-items setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.379999999999999"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = {\n",
    "    'Moscow': [4.2, 2.4],\n",
    "    'St. Petersburg': [-3.4, 2.3],\n",
    "    'New York': [-2.3, -3.1],\n",
    "    'Washington': [3.3, -2.7],\n",
    "}\n",
    "np.array(words['Moscow']).dot(np.array(words['Washington']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word embedding\n",
    "\n",
    "- Factorizing PMI sounds like a promising thing\n",
    "- Nobody knew how to do that properly and it didn't work well\n",
    "- The key was to consider PMI between words and \"contexts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word2vec\n",
    "\n",
    "- Introduced in 2013 and started a revolution\n",
    "- Small program made in totally unreadable C\n",
    "- Consists of two regimes: skip-gram and Continuous Bag of Words (CBoW)\n",
    "- Considers word contexts $\\dots, w_{i-3}, w_{i-2}, w_{i-1}, \\mathbf{w_{i}}, w_{i+1}, w_{i+2}, w_{i+3}, \\dots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Skip-gram\n",
    "\n",
    "$\\dots, w_{i-3}, w_{i-2}, w_{i-1}, \\mathbf{w_{i}}, w_{i+1}, w_{i+2}, w_{i+3}, \\dots$\n",
    "- For any context, $w_i$ is fixed\n",
    "- We try to fit $w_{j}$ so that it is similar to $w_i$\n",
    "- Learn to predict the context given a word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Continuous Bag of Words\n",
    "\n",
    "$\\dots, w_{i-3}, w_{i-2}, w_{i-1}, \\mathbf{w_{i}}, w_{i+1}, w_{i+2}, w_{i+3}, \\dots$\n",
    "\n",
    "- For any context, everything but $w_i$ is fixed\n",
    "- We try to ... vector of $w_i$ to the vector of context\n",
    "- Learn to predict the word given a context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What do we get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.25585938, -0.02209473,  0.02905273,  0.05444336, -0.07421875,\n",
       "        0.35351562, -0.06347656,  0.14453125,  0.07226562,  0.10009766,\n",
       "       -0.18261719, -0.22851562,  0.02209473, -0.22070312,  0.19140625,\n",
       "        0.19140625, -0.16699219,  0.16796875,  0.29492188, -0.18066406,\n",
       "       -0.01452637,  0.10742188, -0.16503906,  0.29882812,  0.12988281,\n",
       "       -0.1171875 , -0.16796875,  0.1015625 ,  0.04492188, -0.12060547,\n",
       "        0.07470703, -0.34765625, -0.10107422,  0.38085938, -0.20605469,\n",
       "       -0.07470703, -0.10839844,  0.18652344,  0.20117188, -0.02124023,\n",
       "        0.28515625, -0.09277344,  0.13964844,  0.05786133,  0.26757812,\n",
       "       -0.15039062, -0.08544922,  0.19238281,  0.08007812,  0.06396484],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_vector('machine')[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Algebra of vectors\n",
    "\n",
    "The vectors we learn in word2vec are actually following some algebraic rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('teaching', 0.6601868271827698),\n",
       " ('learn', 0.6365276575088501),\n",
       " ('Learning', 0.6208059191703796),\n",
       " ('reteaching', 0.5809674263000488),\n",
       " ('learner_centered', 0.5738571286201477),\n",
       " ('emergent_literacy', 0.5706571340560913),\n",
       " ('teach', 0.5704740881919861),\n",
       " ('kinesthetic_learning', 0.5656599998474121),\n",
       " ('learners', 0.5508513450622559),\n",
       " ('learing', 0.5438815355300903)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['learning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning embeddings\n",
    "\n",
    "- Once we have the objective (skip-gram or CBoW) it is possible to employ optimization\n",
    "- Optimization refines the embeddings so that they fit together better\n",
    "- In the end, you get a lookup table (word -> vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### fastText\n",
    "\n",
    "- Very similar thing to word2vec\n",
    "- Consider character n-grams \n",
    "- This enables capturing some language structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cha', 'har', 'ara', 'rac', 'act', 'cte', 'ter']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'character'\n",
    "[word[i:i+3] for i in range(len(word) - 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PMI?\n",
    "\n",
    "- All the methods for finding word embeddings are doing something similar\n",
    "- It is some equivalent of the PMI matrix factorization\n",
    "- Not really an important issue for practicioners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word embeddings in practice\n",
    "\n",
    "- One doesn't really need to train word embeddings\n",
    "- Just google 'pretrained word2vec' or 'pretrained fasttext' and enjoy the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Practical considerations\n",
    "\n",
    "- To embed sentences just sum the word embeddings\n",
    "- Do not forget to normalize the embeddings\n",
    "- Each component of embedding is important, hints for neural networks and linear models\n",
    "- You can actually embed URLs the same way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Next class\n",
    "\n",
    "- Non-parametric methods\n",
    "- k nearest neighbor\n",
    "- k-means"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
