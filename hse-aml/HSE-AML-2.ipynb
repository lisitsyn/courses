{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "import random\n",
    "\n",
    "random.seed(137)\n",
    "rest = random.random()\n",
    "\n",
    "def weight(word):\n",
    "    # overfitted\n",
    "    if word == 'lerxst@wam.umd.edu':\n",
    "        return 100.0\n",
    "    if word == 'car':\n",
    "        return random.random()\n",
    "    if word == 'dog':\n",
    "        return - random.random()\n",
    "    return random.random()\n",
    "\n",
    "def has(word, text):\n",
    "    return word in text \n",
    "\n",
    "def feature(index):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Applied Machine Learning\n",
    "\n",
    "## Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap\n",
    "\n",
    "- We have some dataset\n",
    "- We identify the problem and define the loss function\n",
    "- Then we minimize the total loss (empirical risk, or objective) using available (training) data\n",
    "- We vary parameters to minimize the objective function\n",
    "- The minimizing parameters are then used to predict unknown values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A text classification problem\n",
    "\n",
    "Lets consider the **20 newsgroups** dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec.autos\n",
      "----\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be \n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "data = fetch_20newsgroups()\n",
    "text, label = data['data'][0], data['target_names'][data['target'][0]]\n",
    "print(label)\n",
    "print('----')\n",
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A linear model for classification\n",
    "\n",
    "Let us consider a function that tells if the `text` comes from `rec.autos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4727164112007762"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight('car')*has('car', text) + weight('and')*has('and', text) + rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively say `car` is `0` and `dog` is `1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6520663252475314"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight(0)*feature(0) + weight(1)*feature(1) + rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "How do we find those `weight` ($w$) for all the words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient Descent\n",
    "\n",
    "- Last time we used `opt.fmin` and it magically found the solution\n",
    "- The method is simple though\n",
    "- Start with random weights $w_0$\n",
    "- Iterate: $w_{i+1} = w_{i} - \\alpha \\times \\nabla \\mathsf{objective}(w_i)$\n",
    "- Approximate gradient: $\\nabla f(x) \\sim \\frac{f(x+\\epsilon) - f(x-\\epsilon)}{2\\epsilon}$\n",
    "- All we need to know is the gradient of objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient of loss\n",
    "\n",
    "- Last time we considered a regression problem and used $(y-p)^2$\n",
    "- The gradient w.r.t $p$ is obvious: $- 2 (y - p)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient check\n",
    "\n",
    "How can we ensure the gradient is correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.39999999999999997, -0.400000000000001)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss(y, p):\n",
    "    return (y-p)**2\n",
    "\n",
    "def gradient(y, p):\n",
    "    return -2*(y-p)\n",
    "\n",
    "p = 0.1\n",
    "y = 0.3\n",
    "eps = 0.001\n",
    "gradient(y, p), (loss(y, p+eps) - loss(y, p-eps)) / (2*eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial 0.42717884123187067\n",
      "0 0.35087153649274827\n",
      "1 0.3203486145970993\n",
      "2 0.30813944583883973\n",
      "3 0.3032557783355359\n",
      "4 0.30130231133421437\n",
      "5 0.30052092453368573\n",
      "6 0.30020836981347426\n",
      "7 0.3000833479253897\n",
      "8 0.3000333391701559\n",
      "9 0.3000133356680624\n"
     ]
    }
   ],
   "source": [
    "current_p = random.random()\n",
    "\n",
    "\n",
    "print('Initial', current_p)\n",
    "alpha = 0.3 # learning rate\n",
    "for i in range(10):\n",
    "    current_p = current_p - alpha*gradient(y, current_p)\n",
    "    print(i, current_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification loss\n",
    "\n",
    "- We will use something called **logistic loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 144.26950408889635)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss(y, p):\n",
    "    return np.log2(1.0 + np.exp(-y*p))\n",
    "    \n",
    "loss(+1, 100.0), loss(+1, -100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic Regression in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 19.16157655, -19.59912539,  -0.43754883,  -0.43754883]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sklearn.linear_model.SGDClassifier(loss='log')\n",
    "# feature 1 = car\n",
    "# feature 2 = 3d\n",
    "# feature 3 = and\n",
    "# feature 4 = with\n",
    "example_1 = [1, 0, 1, 1]\n",
    "label_1 = [1] # rec.autos\n",
    "example_2 = [0, 1, 1, 1]\n",
    "label_2 = [0] # comp.graphics\n",
    "model.fit([example_1, example_2], np.ravel([label_1, label_2]))\n",
    "model.coef_ # weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overfitting\n",
    "\n",
    "- We can always come up with a model that fits data perfectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight('lerxst@wam.umd.edu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For some reason that's not what we want. Why?\n",
    "- First, we need to measure if such a thing happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Splitting the data\n",
    "\n",
    "- Obviously we should not test what we fit against\n",
    "- We should fit (train) the model on some part of data\n",
    "- Next, we check the model against the rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Leave-one-out\n",
    "\n",
    "- Generate as many samples as there are examples\n",
    "- Gives you a good estimate if you don't have a lot of data\n",
    "- Gets impractical on huge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4] [0]\n",
      "[0 2 3 4] [1]\n",
      "[0 1 3 4] [2]\n",
      "[0 1 2 4] [3]\n",
      "[0 1 2 3] [4]\n"
     ]
    }
   ],
   "source": [
    "loo = sklearn.model_selection.LeaveOneOut()\n",
    "for train, test in loo.split([1,2,3,4,5]):\n",
    "    print(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross validation\n",
    "\n",
    "- Split the dataset into a few (say 5) non-overlapping parts\n",
    "- Four parts go to training data and one part goes to test data\n",
    "- Do the above 5 times to train the model and test it\n",
    "- 5 runs (precision): 0.9, 0.89, 0.91, 0.92, 0.88 -> mean 0.9 \n",
    "- Makes a decent way to *detect* overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross validation in sklearn\n",
    "\n",
    "Let's consider indices of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 3 4] [1 5]\n",
      "[0 1 4 5] [2 3]\n",
      "[1 2 3 5] [0 4]\n"
     ]
    }
   ],
   "source": [
    "xval = sklearn.model_selection.KFold(n_splits=3, shuffle=True, random_state=9)\n",
    "for train, test in xval.split([1,2,3,4,5,6]):\n",
    "    print(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### This thing is an ill-posed problem\n",
    "\n",
    "- A mathematical problem is ill-posed when the solution is not unique\n",
    "- That's exactly the case of regression/classification/...\n",
    "- We need to make the problem well-posed: *regularization*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Structural risk minimization\n",
    "\n",
    "- Structural risk is empirical risk plus regularizer\n",
    "- Instead of minimizing empirical risk we find some tradeoff\n",
    "- Regularizer is a function of model we get\n",
    "- $\\mathsf{objective} = \\mathsf{loss} + \\mathsf{regularizer}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regularizer\n",
    "\n",
    "- A functions that reflects the complexity of a model\n",
    "- What is the complexity of a set of 'if ... then'?\n",
    "- Not obvious for linear model but easy to invent something"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $\\ell_1$ regularizer\n",
    "\n",
    "- $\\ell_1: \\| w \\|_1 = |w(car)| + |w(dog)| + ...$\n",
    "- Derivative is const\n",
    "- Forces weight to be zero if it doesn't hurt performance much \n",
    "- Use if you believe some features are useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = sklearn.linear_model.SGDClassifier(loss='log', penalty='l1');\n",
    "regression_model = sklearn.linear_model.SGDRegressor(penalty='l1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $\\ell_2$ regularizer\n",
    "\n",
    "- $\\ell_2: \\| w \\|_2 = w(car)^2 + w(dog)^2 + ...$\n",
    "- Derivative is linear\n",
    "- Forces weights to get *similar* magnitude if it doesn't hurt performance much\n",
    "- Use if you believe all features are more or less important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = sklearn.linear_model.SGDClassifier(loss='log', penalty='l2');\n",
    "regression_model = sklearn.linear_model.SGDRegressor(penalty='l2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Elastic net\n",
    "\n",
    "- Just a weighted sum of $\\ell_1$ and $\\ell_2$ regularizers\n",
    "- An attempt to get useful properties of both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model = sklearn.linear_model.SGDRegressor(penalty='elasticnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Limitations of linearity\n",
    "\n",
    "- In low-dimensional spaces linear models are not very 'powerful' (can we define that?)\n",
    "- The higher dimensionality, the more powerful linear model becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sparse features\n",
    "\n",
    "- We say features are sparse when most of the values are zero\n",
    "- Examples: visited hosts, movies that user liked, ...\n",
    "- Sparse features are efficient in high-dimensional setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### One hot encoding, hashing trick\n",
    "\n",
    "- One way to encode categorical things like visited websites\n",
    "- We enumerate all the websites\n",
    "- We put 1 to position of every host, 0 otherwise\n",
    "- Hashing trick: instead of enumerating them just hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2132726271299405475\n",
      "14685 65536\n"
     ]
    }
   ],
   "source": [
    "print(hash('the')) # int64\n",
    "print(hash('the') % 2**16, 2**16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hashing vectorizer in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 613153351)\t1.0\n",
      "  (0, 753185237)\t1.0\n",
      "  (1, 318325784)\t1.0\n",
      "  (1, 753185237)\t1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "vectorizer = HashingVectorizer(n_features=1000000000, binary=True, norm=None)\n",
    "features = vectorizer.fit_transform(['hello there', 'Hey there'])\n",
    "print(features)\n",
    "\n",
    "w = [4.3, 9999999999 more of them]\n",
    "w[613153351]*1.0 + w[753185237]*1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### When do we use linear models?\n",
    "\n",
    "- It is definitely the first thing to try if you have some text data\n",
    "- In general a good choice for any sparse data\n",
    "- This approach is pretty much the fastest one\n",
    "- Even if some method outperforms, you still get a good baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Self-assessment questions\n",
    "\n",
    "1. You noticed that your linear model learned a weight of **95.3** for the word `the`. *Is there a problem? Y/N*\n",
    "2. The train loss is **0.43** and the test loss is **0.39**. *Is it an example of ..? a) overfitting b) underfitting c) I don't know*\n",
    "3. You've got basically infinite amounts of data. *Do you have to use regularization? Y/N*\n",
    "4. You believe your dataset is pretty noisy and some features are broken. *You use a) L1 b) L2 c) no regularization* \n",
    "5. Why do we hash words? *a) it's simpler b) it's faster c) it's more reliable*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Homework 1\n",
    "\n",
    "- Load dataset, create linear model, train, and explain the results\n",
    "- The template is provided: `HSE-AML-HW1.ipynb`\n",
    "- Hint: check the code examples for `KFold`, `HashingVectorizer`, `LogisticRegression`"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
